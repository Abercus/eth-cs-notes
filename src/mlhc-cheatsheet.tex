\documentclass[11pt]{article}

% Packages
\usepackage{bookmark}
%\hypersetup{bookmarks,bookmarksopen,bookmarksdepth=2}
\hypersetup{hidelinks}
\usepackage[margin=0.5in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
%\usepackage{scrextend}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage{lipsum}
\usepackage{xargs}
\usepackage[pdftex,dvipsnames]{xcolor}
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\usepackage{titlesec}
\titlespacing*{\section}
{0pt}{0.5em}{0.2em}
%\titleformat{\section}
%  {\normalfont\scshape}{\thesection}{1em}{}

% Todo notes
\newcommandx{\must}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
\newcommandx{\maybe}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}

% List items
\newcommand{\pro}{\item[$+$]}
\newcommand{\con}{\item[$-$]}

% Unordered list
\newcommand{\ulb}{\begin{enumerate}[label=\textbullet,topsep=0pt,itemsep=-0.5ex,partopsep=1ex,parsep=1ex]}
\newcommand{\ule}{\end{enumerate}\vspace{1mm}}

% Ordered list
\newcommand{\olb}{\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]}
\newcommand{\ole}{\end{enumerate}\vspace{1mm}}

% Quotes
\usepackage [english]{babel}
\usepackage [autostyle, english = american]{csquotes}
\MakeOuterQuote{"}

% Multicolumn
\newcommand{\mcb}[1]{\begin{multicols}{#1}}
\newcommand{\mce}{\end{multicols}\noindent}

% Minipage
\newcommand{\mpb}[1]{\begin{minipage}[t]{#1}}
\newcommand{\mpe}{\end{minipage}
}
% Norm
\newcommand{\snorm}[1]{||#1||}
\newcommand{\bnorm}[1]{\left\lVert#1\right\rVert}

% Kullback-Leibler divergence
\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
  #1\;\delimsize\|\;#2%
}
\newcommand{\dkl}{D_{KL}\infdivx}
\newcommand{\djs}{D_{JS}\infdivx}

% Shorthand
\newcommand{\R}{\mathbb{R}}
\newcommand{\loss}{\mathcal{L}}
\newcommand{\risk}{\mathcal{L}}
\newcommand{\ra}{\rightarrow}
\newcommand{\goes}{\rightarrow}
\newcommand{\given}{\mid}
\newcommand{\normal}{\mathcal{N}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\Eu}{\mathop{\mathbb{E}}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\rank}{\text{rank}}
\newcommand{\sigm}{\mathop{\text{sigm}}}
\def\BState{\State\hskip-\ALG@thistlm}

\begin{document}

\par In biology, a \textbf{gene} is a sequence of nucleotides in DNA or RNA that codes for a molecule that has a function. The \textbf{genotype} is the part of the genetic makeup of a cell, and therefore of any individual, which determines one of its characteristics (phenotype). The \textbf{genotype–phenotype distinction} is drawn in genetics. "Genotype" is an organism's full hereditary information. "Phenotype" is an organism's actual observed properties, such as morphology, development, or behavior. An \textbf{allele} is a variant form of a given gene. A \textbf{heterozygous} individual is someone who has two different alleles at a locus. A \textbf{homozygous} individual has two identical alleles at a locus.

\section{Support Vector Machines and Kernels for Computational Biology}
\par \textbf{RNA splicing}, in molecular biology, is a form of RNA processing in which a newly made precursor messenger RNA (pre-mRNA) transcript is transformed into a mature messenger RNA (mRNA).
\par \textbf{Kernel trick}: Scalar product in feature space can be computed in input space. Common kernels: polynomial, sigmoid, RBF, normalization... Kernels allow to encode application-specific knowledge. Many kernels for different applications available.
\par String kernel SVMs capable of efficiently dealing with large k-mers k > 10.
\\
\textbf{Spectrum kernel}: position-independent motifs. \textbf{Spectrum Kernel with Mismatches}: Do not enforce strictly exact matches. \textbf{Weighted-degree kernel}: position-dependent motifs. As weighting use $\beta_k = 2\frac{d-k+1}{d(d+1)}$, where d is the maximal match length taken into account. This way the longer matches are weighted less, but they imply many shorter matches. \textbf{Weighted Degree Kernel with Shifts}: partially position-dependent motifs
\par \textbf{SVM scoring function:} SVM decision function is $\alpha$-weighting of training points, but we are interested in weights of features. We can explicitly compute w and use it to rank importance. Explicit representation of w allows (some) interpretation. SVM-w does not reflect the score for a motif as substrings and overlapping strings contribute, too!
\par \textbf{Positional Oligomer Importance Matrices (POIMs)}
\ulb\item Given k-mer z at position j in the sequence, compute expected score $\mathbb{E} [ s(x)  \given  x[j] = z ]$ (for small k)
\item Normalize with expected score over all sequences
\item For large k use differential POIM
\ule
A \textbf{sequence logo} consists of a stack of letters at each position. The relative sizes of the letters indicate their frequency in the sequences. The total height of the letters depicts the information content of the position, in bits. The lowest order POIM (k=1) essentially conveys the same information as is represented in a sequence logo. However, unlike sequence logos, POIMs naturally generalize to higher order nucleotide patterns.
\par A position weight matrix (PWM), also known as a position-specific weight matrix (PSWM) or \textbf{position-specific scoring matrix (PSSM)}, is a commonly used representation of motifs (patterns) in biological sequences. PWMs are often derived from a set of aligned sequences that are thought to be functionally related and have become an important part of many software tools for computational motif discovery.
\par A PWM has one row for each symbol of the alphabet: 4 rows for nucleotides in DNA sequences or 20 rows for amino acids in protein sequences. It also has one column for each position in the pattern. In the first step in constructing a PWM, a basic position frequency matrix (PFM) is created by counting the occurrences of each nucleotide at each position. From the PFM, a position probability matrix (PPM) can now be created by dividing that former nucleotide count at each position by the number of sequences, thereby normalising the values. Most often the elements in PWMs are calculated as log likelihoods. 

\section{Biomedical Natural Language Processing}
\par \textbf{Term frequency (TF)}: the raw count of a term in a document: the number of times that term t occurs in document d. TF suffers from a critical problem: all terms are considered equally important. In fact certain terms have little or no discriminating power in determining relevance. Basic formulation is $f_{t,d} / \sum_{t'} f_{t',d}$
\par \textbf{Document Frequency (DF)}: the number of documents in the collection that contain a term t. Basic formulation of IDF is $\log N / n_t$
\par In information retrieval, tf–idf or TFIDF, short for \textbf{term frequency–inverse document frequency}, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling.
\par \textbf{Brill tagger} (Transformation-Based Learning): a transformation-based process, in the sense that a tag is assigned to each word and changed using a set of predefined rules. In the transformation process, if the word is known, it first assigns the most frequent tag, or if the word is unknown, it naively assigns the tag "noun" to it. Applying over and over these rules, changing the incorrect tags, a quite high accuracy is achieved.
\par \textbf{Why use embeddings?} Reduce dimensionality of representation. Encodes similarity information, useful for other tasks. Learn representations of entities (words) as well as relationships between them.
\par \textbf{Word2Vec:} Train a classifier on a binary prediction task of words occurring in the neighbourhoods of other words, take the learned classifier weights as the word embeddings. Faster and can easily incorporate a new sentence/document or add a word to the vocabulary. \textbf{Continuous Bag-of-Words (CBoW) model}: predict center word from sum of surrounding word vectors. \textbf{Skip-gram model}: predicting surrounding single words from center word (see NLU notes). Take the target word and a neighboring context word as positive examples, randomly sample other words in the lexicon to get negative samples. Normalized dot-product gives \textbf{cosine similarity}. This means we maximise the overlap (via dot product) between a word and the context it appeared in. By transitivity, any other word with a similar context will have a large overlap with the original word. For example, jumps $\sim$ leaps because their context vectors are similar.
\par Combining embeddings with prior knowledge: from analogical reasoning, abstract relationships were translations in the embedded space. Take this idea and extend the concept of context to include "appears in a relationship with" alongside "appears in a sentence with" and represent these new context-relationships as arbitrary affine transformations (basically, matrices).
\par Enforcing similarity: define an energy function $\mathcal{E} (S,R,T)$, energy is low if S is related to T through R is true (R is often non-symmetric). An example  energy function is $\mathcal{E} (S,R,T \given \theta) = -\frac{\mathbf{v}_T \cdot G_R \mathbf{c}_S}{\snorm{\mathbf{v}_T}\snorm{G_R \mathbf{c}_S}}$
\par "Off-task" data helps due to shared semantic information.

\section{Time Series Analysis}
\par Auto-regressive model are suited for stationary time series. A stationary time series is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time. Most statistical forecasting methods are based on the assumption that the time series can be rendered approximately stationary (i.e., "stationarized") through the use of mathematical transformations.
\par \textbf{Classification of AR models} $AR(p)$: observed, continuous, RNN: hidden, continuous, Markov chain: observed, discrete, HMM: hidden, discrete

\section{Survival Analysis}
\par The \textbf{log-rank test} compares the survival times of two or more groups. The null hypothesis for a log-rank test is that the groups have the same survival.
\\
\textbf{Censoring}:
\ulb
\item Left censoring – a data point is below a certain value but it is unknown by how much.
\item Interval censoring – a data point is somewhere on an interval between two values.
\item Right censoring – a data point is above a certain value but it is unknown by how much.
\item Type I censoring occurs if an experiment has a set number of subjects or items and stops the experiment at a predetermined time, at which point any subjects remaining are right-censored.
\item Type II censoring occurs if an experiment has a set number of subjects or items and stops the experiment when a predetermined number are observed to have failed; the remaining subjects are then right-censored.
\item Random (or non-informative) censoring is when each subject has a censoring time that is statistically independent of their failure time. The observed value is the minimum of the censoring and failure times; subjects whose failure time is greater than their censoring time are right-censored.
\ule
\ulb
\item The lifetime distribution function, conventionally denoted F, is defined as the complement of the survival function: $F(t) = P(T \leq t) = 1 - S(t)$
\item If F is differentiable then the derivative, which is the density function of the lifetime distribution, is conventionally denoted f: $f(t) = F'(t) = \frac{d}{dt}F(t)$
\item The function f is sometimes called the event density; it is the rate of death or failure events per unit time.
\ule
\par The hazard function, $h(t)$, is the instantaneous rate at which events occur, given no previous events. (The hazard function, conventionally denoted $\lambda$ , is defined as the event rate at time t conditional on survival until time t or later (that is, $T \geq t$).)
\par Proportional hazards models are a class of survival models in statistics. In a proportional hazards model, the unique effect of a unit increase in a covariate is multiplicative with respect to the hazard rate. For example, taking a drug may halve one's hazard rate for a stroke occurring, or, changing the material from which a manufactured component is constructed may double its hazard rate for failure.
\par Kaplan-Meier curves and log-rank tests are most useful when the predictor variable is categorical (e.g., drug vs. placebo), or takes a small number of values (e.g., drug doses 0, 20, 50, and 100 mg/day) that can be treated as categorical. The log-rank test and KM curves don't work easily with quantitative predictors such as gene expression, white blood count, or age. For quantitative predictor variables, an alternative method is \textbf{Cox proportional hazards regression analysis}.

\section{Privacy Preserving Methods for ML in Healthcare}
\par Anonymization refers to irreversibly severing a data set from the identity of the data contributor in a study to prevent any future re-identification, even by the study organizers under any condition. There's no re-identification of anonymized records, because the links back to the subjects are irreversibly broken. De-identification is also a severing of a data set from the identity of the data contributor, but may include preserving identifying information which can only be re-linked by a trusted party in certain situations.
\par Why generate synthetic data?
\ulb
\item Data could be shared and published without privacy concerns (e.g. scientific reproducibility)
\item Data can be used to augment or enrich similar datasets
\item Represents an alternative approach to build predictive systems
\item Can benefit medical community for use in medical training simulator
\ule
TRTS is not as interesting as the TSTR case as it cannot diagnose mode collapse. Evaluating synthetic datasets: Mechanical Turks when no domain knowledge is needed, Inception score for images
\par \textbf{Differential privacy} addresses the case when a trusted data curator wants to release some statistic over its data without revealing information about a particular value itself. It is a constraint on the algorithms used to publish aggregate information about a statistical database which limits the privacy impact on individuals whose information is in the database. Roughly, an algorithm is differentially private if an observer seeing its output cannot tell if a particular individual's information was used in the computation. The most general mechanism is known as the Laplace mechanism, which adds Laplace noise to data so that everything an adversary receives becomes noisy and imprecise, and so it is much more difficult to breach privacy (if it is feasible at all). Challenges in DP: The more information you intend to "ask" of your database, the more noise has to be injected in order to minimize the privacy leakage. Once data has been leaked, it's gone. The total allowed leakage is often referred to as a "privacy budget", and it determines how many queries will be allowed (and how accurate the results will be). "Estimation from repeated queries" is also one of the fundamental limitations of differential privacy.

\section{Interpretability of ML Models}
\par Random forests tries to improve on bagging by "de- correlating" the trees.
\par \textbf{Sensitivity Analysis of Individual Variables}: It is a global explanation method. We examine what impact each feature has on the model's prediction. Possible transformations that can be done during analysis are sampling uniformly from the feature distribution, permutation of the feature values, replacing the values by mean or zero.
\par \textbf{Mean Decrease in Impurity (MDI)}, (also called as Gini Importance) is defined as the total decrease in node impurity (weighted by the probability of reaching that node (which is approximated by the proportion of samples reaching that node)) averaged over all trees of the ensemble.
\par Standard Encoder-Decoder framework: $q,f,g$ nonlinear functions, $s_t$ is the hidden state of the decoder RNN. Here, the context vector $c$ is the same for $\forall y_t$.
\par \textbf{Encoder-Decoder with Bahdanau attention:} $a$ is so-called alignment model, jointly trained with all other components. Unlike existing encoder-decoder models, probability in $g$ is conditioned on a distinct context vector $c_i$ for each target word $y_i$. Probability $\alpha_{ij}$ (or energy $e_{ij}$ for that reason) reflects the importance of the annotation $h_{j}$ w.r.t. previous hidden state $s_{i-1}$ in deciding the next state $s_i$ and generating $y_i$.
\par Show, attend and tell (two mechanism for obtaining context vectors from annotation vectors):
\ulb
\item Hard (stochastic) attention: returns a sample from every point in time, based upon a categorical distribution (of locations) parametrized by $\alpha$
\item Soft attention: takes the expectation of the context vector directly
\ule

\section{Appendix}
\vspace{-0.5cm}
\begin{small}
\mcb{2}
\ulb
\item sensitivity, recall, hit rate, or true positive rate (\textbf{TPR})
$$ \text{TPR} = \frac{\text{TP}}{P} = \frac{\text{TP}}{\text{TP + FN}} = 1 - \text{FNR} $$

\item precision, positive predictive value (\textbf{PPV})
$$ \text{PPV} = \frac{\text{TP}}{\text{TP + FP}} = 1 - \text{FDR} $$

\item miss rate, false negative rate (\textbf{FNR})
$$ \text{FNR} = \frac{\text{FN}}{P} = \frac{\text{FN}}{\text{FN + TP}} = 1 - \text{TPR} $$

\item false discovery rate (\textbf{FDR})
$$ \text{FDR} = \frac{\text{FP}}{\text{FP + TP}} = 1 - \text{PPV} $$

\item accuracy (\textbf{ACC})
$$ \text{ACC} = \frac{\text{TP + TN}}{P + N} = \frac{\text{TP + TN}}{\text{TP + TN + FP + FN}} $$

\item specificity, selectivity or true negative rate (\textbf{TNR})
$$ \text{TNR} = \frac{\text{TN}}{N} = \frac{\text{TN}}{\text{TN + FP}} = 1 - \text{FPR} $$

\item negative predictive value (\textbf{NPV})
$$ \text{NPV} = \frac{\text{TN}}{\text{TN + FN}} = 1 - \text{FOR} $$

\item fall-out, false positive rate (\textbf{FPR})
$$ \text{FPR} = \frac{\text{FP}}{N} = \frac{\text{FP}}{\text{FN + TN}} = 1 - \text{TNR} $$

\item false omission rate (\textbf{FOR})
$$ \text{FOR} = \frac{\text{FN}}{\text{FN + TN}} = 1 - \text{NPV} $$

\item \textbf{F1 score}
$$ F_1 = 2 \cdot \frac{\text{PPV} \cdot \text{TPR}}{\text{PPV + TPR}} = \frac{\text{2TP}}{\text{2TP + FP + FN}} $$

\ule
\mce
\end{small}
\vspace{-0.5cm}
\par It can be more flexible to predict probabilities of an observation belonging to each class in a classification problem rather than predicting classes directly. This is required when using models where the cost of one error outweighs the cost of other types of errors. For example, in a smog prediction system, we may be far more concerned with having low false negatives than low false positives. A false negative would mean not warning about a smog day when in fact it is a high smog day, leading to health issues in the public that are unable to take precautions. A false positive means the public would take precautionary measures when they didn't need to.
\par The \textbf{ROC curve} is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease. Area under ROC Curve (AUROC) is robust to imbalanced classes (for example, mortality has 2\% positive examples).
\par Precision can be seen as a measure of exactness or quality, whereas recall is a measure of completeness or quantity. ROC curves are appropriate when the observations are balanced between each class, whereas \textbf{precision-recall curves} are appropriate for imbalanced datasets. Key to the calculation of precision and recall is that the calculations do not make use of the true negatives. It is only concerned with the correct prediction of the minority class, class 1. Area under Precision-Recall Curve (AUPRC) quantifies the tradeoff between sensitivity and false discovery, which is relevant in a clinical setting.
\par One reason to use the \textbf{logarithmic scale} is to respond to skewness towards large values; i.e., cases in which one or a few points are much larger than the bulk of the data. log scales allow a large range to be displayed without small values being compressed down into bottom of the graph. Another reason is to show percent change or multiplicative factors. In linear scale, even if the performance in percentage terms has been fairly constant a graph of the funds will appear to have grown most rapidly at the right hand end. With a logarithmic scale a constant percentage change is seen as a constant vertical distance so a constant growth rate is seen as a straight line. That is often a substantial advantage. In short, a logarithmic axis linearizes compound interest and exponential growth. A logarithmic axis is useful for plotting ratios. Ratios are intrinsically asymmetrical, but ratios are symmetrical on a log scale.
\par \textbf{Digital phenotyping} is a multidisciplinary field of science, defined as the "moment-by-moment quantification of the individual-level human phenotype in situ using data from personal digital devices", in particular smartphones.

\end{document}